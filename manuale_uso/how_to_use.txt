Per usare questa estensione è necessario seguire alcuni passaggi
fondamentali per prepare tutto il necessario al fine di far funzionare
al meglio il codice. Alcuni passaggi differiscondo tra windows e mac,
sarà specificato nella guida. L'estensione funziona su firefox e sfrutta
ollama (tramite API e modello custom) per classificare la policy privacy
che silentemente accettiamo visitando la pagina.

Passaggi preliminari:

- Installare postman o metodo equivalente per testare le API https://www.postman.com/downloads/
- Installare firefox https://www.mozilla.org/it/firefox/new/
- Installare ollama in base al proprio SO https://ollama.com/download

Passaggi successivi:

- Creare un modello LLM custom su ollama:

    1. Definire che modello usare: in base al proprio Hardware sceglire
    se utilizzare ilama3.1 (8B parametri) o gemma2 (2B parametri). Ci
    sono molti modelli supportati per ollama, verificare questa guida per
    approfondire il punto: https://github.com/ollama/ollama?tab=readme-ov-file
    oppure https://ollama.com/library per la lista completa. Scelto il modello
    scarichiamolo sul pc e testiamolo.

    2. Dobbiamo creare un modello custom partendo da quello scelto, possiamo
    seguire questa guida https://medium.com/@sumudithalanz/unlocking-the-power-of-large-language-models-a-guide-to-customization-with-ollama-6c0da1e756d9
    L'obiettivo e creare un modello con uno dei modelfile nella cartella /LLM, questo
    aumenta la possibilità di ricevere una risposta il linea a LLM/LLM_output.json, che
    poi verrà trasformata in LLM/Mod_output.json per l'estensione.
    Nello specifico i comandi sono:
        1. "cd dir/to/LLM" posizioniamoci con il terminare nella cartella /LLM
        2. "ollama create new-gemma --file gemma3.modelfile" creiamo
            il modello new-gemma con il model file nella cartella LLM.
            Processo analogo con altri modelli, l'importante è mantenere
            le info di SYSTEM e PARAMETER presenti nei file forniti nel codice.
        3. "ollama list" assicuriamoci che il modello sia stato creato

    3. Importiamo in postman postman/LLM API v 1.0.0.json, collection con alcune API
    utili per verificare se ollama e il modello stanno funzionando correttamente. Usiamo
    l'api generate sostuituendo in "model" il nome effettivo del modello creato. Verifichiamo
    che risponda correttamente o che per lo meno risponda.

- Modifiche necessarie al codice/preparazione avvio:

    1. Nel file config.json è possibile impostare l'endpoint di ollama, il nome del modello
    e la possibilità di utilizzare una API fittizia che quindi risponde con valori casuali.
    NB. questa funzione che fa le veci del modello funziona solo per la parte di analisi
    delle pagine e non per la parte di prompting.

    2. Ollama di base non accetta chiamate alle sue API che provengano da origini diverese
    da localhost, quindi modifichiamo questa impostazione permettendo a firefox extension
    di poter chiamare l'api e non ricevere un 403 (dobbiamo creare una variabile d'ambiente):
    su mac: launchctl setenv OLLAMA_ORIGINS "*"
    su widnows: aggiungere questa var d'ambiente OLLAMA_ORIGINS=*
    Per maggiori infomrazioni https://medium.com/dcoderai/how-to-handle-cors-settings-in-ollama-a-comprehensive-guide-ee2a5a1beef0

    3. Riavviare Ollama

- Utilizzo

    1. Aprire firefox e raggiungere la pagina about:debugging poi cliccare su "This Firefox"
    e poi su load temporary add-on, quindi selezionare un file di primo livello dell'estensione.
    Aprire anche i log dell'estensione per maggiori verifiche immagine:
    
    2. Aprire le 3 pagine di test nella cartella test, attendere il caricamento e verificare i log
    per maggiori informazioni.

    3. Vedere le immagini commentate per maggiore compresione di uso dell'estensione